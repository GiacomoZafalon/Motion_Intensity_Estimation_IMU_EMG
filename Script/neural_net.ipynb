{"cells":[{"cell_type":"code","execution_count":null,"id":"534340dc-c395-4cad-9a06-a8d4c87f06eb","metadata":{"id":"534340dc-c395-4cad-9a06-a8d4c87f06eb","tags":[]},"outputs":[],"source":["import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import pandas as pd\n","import math\n","import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Flatten\n","import torch.optim as optim"]},{"cell_type":"code","execution_count":null,"id":"c05345d3-4abe-4e5e-b4ce-e75279e0a081","metadata":{"id":"c05345d3-4abe-4e5e-b4ce-e75279e0a081","tags":[]},"outputs":[],"source":["# IMU-Based Energy Expenditure Estimation for Various Walking Conditions Using a Hybrid CNNâ€“LSTM Model\n","# conv1d(64, 3)\n","# maxpooling1d(64, 3)\n","# 2 LSTM 64 hidden units\n","# flatten layer\n","# fully connected layer\n","# output"]},{"cell_type":"code","execution_count":null,"id":"WS2Ke99Ex0dU","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18527,"status":"ok","timestamp":1718203037615,"user":{"displayName":"Giacomo Zafalon","userId":"02253282456549090827"},"user_tz":-180},"id":"WS2Ke99Ex0dU","outputId":"9a1bf541-247a-41b5-8a80-0b22b88dc41c"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"id":"406b5df7-8fa1-4690-b807-9990f52f0411","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6788,"status":"ok","timestamp":1718191946717,"user":{"displayName":"Giacomo Zafalon","userId":"02253282456549090827"},"user_tz":-180},"id":"406b5df7-8fa1-4690-b807-9990f52f0411","outputId":"edc4a9c8-647a-4929-d6a0-e460f985bdc2","tags":[]},"outputs":[],"source":["class DataProcessor:\n","    def __init__(self, person, weight, attempt):\n","        self.person = person\n","        self.weight = weight\n","        self.attempt = attempt\n","\n","    def load_imu_data(self, person, weight, attempt):\n","        imu_path = f'/content/drive/MyDrive/Colab_Notebooks/Master_Thesis/Neural_Network/Dataset/P{person}/W{weight}/A{attempt}/imu/data_neural.csv'\n","        imu_data = pd.read_csv(imu_path)  # Load CSV data using pandas\n","\n","        imu_chunks = []\n","        num_timesteps = imu_data.shape[0]\n","        num_chunks = num_timesteps // 100\n","        remainder = num_timesteps % 100\n","\n","        for i in range(num_chunks):\n","            chunk = imu_data.iloc[i * 100: (i + 1) * 100]\n","            imu_chunks.append(torch.tensor(chunk.values))\n","\n","        if remainder > 20:\n","            last_chunk = imu_data.iloc[-100:]\n","            imu_chunks.append(torch.tensor(last_chunk.values))\n","\n","        return imu_chunks\n","\n","    def load_emg_data(self, person, weight, attempt):\n","        emg_path = f'/content/drive/MyDrive/Colab_Notebooks/Master_Thesis/Neural_Network/Dataset/P{person}/W{weight}/A{attempt}/emg/emg_label.csv'\n","        emg_data = pd.read_csv(emg_path)  # Load CSV data using pandas\n","        emg_tensor = torch.tensor(emg_data.iloc[:, 1])  # Convert DataFrame to tensor\n","        return emg_tensor\n","\n","    def process_data(self):\n","        all_data = []\n","\n","        for p in range(1, self.person + 1):\n","            for w in range(1, self.weight + 1):\n","                for a in range(1, self.attempt + 1):\n","                    emg_data = self.load_emg_data(p, w, a)\n","                    imu_data = self.load_imu_data(p, w, a)\n","                    for chunk in range(len(imu_data)):\n","                        imu_chunk = imu_data[chunk]\n","                        all_data.append((imu_chunk, emg_data))\n","\n","            print(f'Person {p}/{self.person} done')\n","\n","        return all_data\n","\n","# Example usage:\n","person = 10\n","weight = 5\n","attempt = 6\n","\n","processor = DataProcessor(person, weight, attempt)\n","all_data = processor.process_data()\n","print(len(all_data))\n","for i in range(len(all_data)):\n","    data, label = all_data[i]\n","    print(data.shape, label)\n","print(data)\n","print(label)\n","print(all_data[-1])"]},{"cell_type":"code","execution_count":null,"id":"2c95149b-9a12-4198-93ee-08a17ba33dcc","metadata":{"id":"2c95149b-9a12-4198-93ee-08a17ba33dcc","tags":[]},"outputs":[],"source":["class CNN_LSTM(nn.Module):\n","    def __init__(self, num_data, out_channels=64, kernel_conv=1, kernel_pool=1, hidden_lstm=64, layers_lstm=1, num_classes=5, time_steps=100):\n","        super(CNN_LSTM, self).__init__()\n","        self.batch_size = batch_size\n","\n","        # Linear layer\n","        self.linear1 = nn.Linear(time_steps*num_data, num_data)\n","\n","        # 1D Convolutional Layer\n","        self.conv1 = nn.Conv1d(in_channels=num_data, out_channels=64, kernel_size=kernel_conv).double()\n","\n","        # Max Pooling Layer\n","        self.pool1 = nn.MaxPool1d(kernel_size=kernel_pool).double()\n","\n","        # LSTM layers\n","        self.lstm = nn.LSTM(out_channels, hidden_lstm, layers_lstm, batch_first=True).double()\n","\n","        # Fully connected layer for regression\n","        self.fc1 = nn.Linear(out_channels, num_classes).double() # Output dimension is 1 for regression\n","\n","        self.init_weights()\n","\n","    def init_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv1d) or isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 1e-1)\n","                nn.init.zeros_(m.bias)\n","\n","    def forward(self, x): # shape (batch_size, time_steps, num_data) = (2, 100, 13)\n","        # print('1', np.shape(x))\n","        x = x.view(batch_size, -1) # shape (batch_size, 1*time_steps*num_data) = (2, 1300)\n","        # print('2', np.shape(x))\n","        x = self.linear1(x.float()) # shape (batch_size, 1*num_data) = (2, 13)\n","        # print('3', np.shape(x))\n","        x = x.view(batch_size, 1, 13) # shape (batch_size, 1, 1, num_data) = (2, 1, 13)\n","        # print('4', np.shape(x))\n","        x = F.relu(x) # shape (batch_size, num_sensors, 1, num_data) = (2, 1, 13)\n","\n","        x = x.double()\n","        x = x.permute(0, 2, 1) # shape (batch_size, num_data, num_sensors) = (2, 13, 1)\n","        # print('5', np.shape(x))\n","        x = self.conv1(x) # shape (batch_size, out_channels, _) = (2, 64, 1)\n","        # print('6', np.shape(x))\n","        x = self.pool1(x) # shape (batch_size, out_channels, 1) = (2, 64, 1)\n","        # print('7', np.shape(x))\n","        x = F.relu(x) # shape (batch_size, out_channels, 1) = (2, 64, 1)\n","\n","        x = x.squeeze(2) # shape (batch_size, out_channels) = (2, 64)\n","        # print('8', np.shape(x))\n","        x, _ = self.lstm(x) # shape (batch_size, out_channels) = (2, 64)\n","\n","        x = self.fc1(x).float() # shape (batch_size, num_classes) = (2, 5)\n","        # print('9', np.shape(x))\n","        output = F.softmax(x, dim=1) # shape (batch_size, num_classes) = (2, 5)\n","        # print('10', np.shape(output))\n","        return output"]},{"cell_type":"code","execution_count":null,"id":"d91e0ca4-9936-4492-bcee-c8a558e6b25d","metadata":{"id":"d91e0ca4-9936-4492-bcee-c8a558e6b25d","tags":[]},"outputs":[],"source":["import sys\n","def train(model, train_loader, time_steps, criterion, optimizer, epochs=5, batch_size=1):\n","    model.train()\n","    losses = []\n","    total_batches = math.ceil(len(train_loader.dataset) / batch_size)\n","    for epoch in range(epochs):\n","        running_loss = 0.0\n","        print('-----------EPOCH %d-----------' % (epoch + 1))\n","        for i, data in enumerate(train_loader):\n","            if len(train_loader.dataset) % batch_size != 0 and i == total_batches - 1:\n","                break\n","            # print(i)\n","            inputs, labels = data\n","            optimizer.zero_grad()\n","            labels = labels.view(-1)\n","            inputs = inputs.float()\n","            outputs = model(inputs)\n","            # outputs = outputs.float()\n","            # labels = labels.long()\n","            # print(outputs)\n","            # outputs = torch.argmax(outputs, dim=1)\n","            # print('output:', outputs)\n","            # print('labels:', labels)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","            running_loss += loss.item()\n","            # losses.append(running_loss / total_batches)\n","            # running_loss += loss.item() * inputs.size(0)\n","            if (i + 1) % 20 == 0:\n","                print('[episode: %d] loss: %f' % (i + 1, running_loss / 2))\n","                losses.append(running_loss / 2)\n","                running_loss = 0.0\n","\n","    print(outputs, labels)\n","    plt.plot(losses)\n","    plt.xlabel('Iterations')\n","    plt.ylabel('Loss')\n","    plt.title('Training Loss Curve')\n","    plt.show()\n","\n","def test(model, test_loader, batch_size=1):\n","    model.eval()\n","    total_error = 0\n","    total_batch = 0\n","    # total_error2 = 0\n","    total_batches = math.ceil(len(test_loader.dataset) / batch_size)\n","    # print(total_batches)\n","    # print(len(test_loader.dataset) % batch_size)\n","    with torch.no_grad():\n","        for i, data in enumerate(test_loader):\n","            # print('i', i)\n","            # print('tot', total_batches)\n","            # print('%', len(test_loader.dataset) % batch_size)\n","            if len(test_loader.dataset) % batch_size != 0 and i == total_batches - 1:\n","                print('here')\n","                break\n","            inputs, labels = data\n","            inputs = inputs.float()\n","            labels = labels.long()\n","            labels = labels.view(-1)\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            total_error += loss.item()\n","            total_batch += 1\n","            # batch_error = torch.abs(outputs.flatten() - labels.flatten()) / labels.flatten()\n","            # batch_error2 = torch.abs(outputs.flatten() - labels.flatten())\n","            # total_error += batch_error.mean().item()\n","            # total_error2 += batch_error2.mean().item()\n","            if (i + 1) % 20 == 0:\n","                print(outputs, labels)\n","    # print('Average accuracy: %f %% ' % (100 - total_error / total_batches * 100))\n","    # print('Average error between prediction and label: %f' % (total_error2 / total_batches))\n","    average_loss = total_error / total_batch\n","    print('Average test loss: {:.4f}'.format(average_loss))"]},{"cell_type":"code","execution_count":null,"id":"fbdf319d","metadata":{"id":"fbdf319d"},"outputs":[],"source":["data, label = all_data[0]\n","data_list = []\n","label_list = []\n","for i in range(len(all_data)):\n","  data, label = all_data[i]\n","  data_list.append(data)\n","  label_list.append(label)\n","\n","num_rows = len(data_list)\n","indices = torch.randperm(num_rows).tolist()\n","shuffled_data = [data_list[i] for i in indices]\n","shuffled_labels = [label_list[i] for i in indices]\n","\n","shuffled_dataset = (shuffled_data, shuffled_labels)"]},{"cell_type":"code","execution_count":null,"id":"d33QPfgM0cL5","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":464,"status":"ok","timestamp":1718193476954,"user":{"displayName":"Giacomo Zafalon","userId":"02253282456549090827"},"user_tz":-180},"id":"d33QPfgM0cL5","outputId":"97248829-1cfd-45ea-e763-2643b093f75d"},"outputs":[],"source":["print(shuffled_data[47], shuffled_labels[47])\n","print(np.shape(shuffled_data[47]))"]},{"cell_type":"code","execution_count":null,"id":"275eb8c0-1bad-48db-8d39-b908a5e05a68","metadata":{"cellView":"code","id":"275eb8c0-1bad-48db-8d39-b908a5e05a68","tags":[]},"outputs":[],"source":["# @title Testo del titolo predefinito\n","# data, labels = dataset[:, :]\n","# num_rows = data.shape[0]\n","# indices = torch.randperm(num_rows)\n","# shuffled_data = data[indices]\n","# shuffled_labels = labels[indices]\n","# shuffled_dataset = shuffled_data, shuffled_labels\n","\n","# data, label = dataset[:]\n","\n","batch_size = 2\n","\n","train_set = [(shuffled_data[i], shuffled_labels[i]) for i in range(np.int64(len(all_data)/2))]\n","train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","test_set = [(shuffled_data[i], shuffled_labels[i]) for i in range(np.int64(len(all_data)/2), len(all_data))]\n","test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n","\n","# train_set = [(dataset[i]) for i in range(np.int64(len(dataset)/2))]\n","# train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=2)\n","\n","# test_set = [(dataset[i]) for i in range(np.int64(len(dataset)/2), len(dataset))]\n","# test_loader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=2)\n"]},{"cell_type":"code","execution_count":null,"id":"846c7f8d-62cd-4d3e-af19-7ef7039928b7","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":6211,"status":"ok","timestamp":1718195259572,"user":{"displayName":"Giacomo Zafalon","userId":"02253282456549090827"},"user_tz":-180},"id":"846c7f8d-62cd-4d3e-af19-7ef7039928b7","outputId":"24e8011f-362a-410c-e424-d4b4bc3301ca","tags":[]},"outputs":[],"source":["num_data = np.shape(shuffled_data)[2]\n","time_steps = np.shape(shuffled_data)[1]\n","model = CNN_LSTM(num_data, layers_lstm=2)\n","# criterion = nn.MSELoss()\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n","\n","train(model, train_loader, time_steps, criterion, optimizer, epochs=3, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"29feab3a-1bda-44b8-9ce7-91c5a1970f54","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1289,"status":"ok","timestamp":1718195656173,"user":{"displayName":"Giacomo Zafalon","userId":"02253282456549090827"},"user_tz":-180},"id":"29feab3a-1bda-44b8-9ce7-91c5a1970f54","outputId":"7f0049b1-1dd6-4d97-e968-7c29951a2e1f","tags":[]},"outputs":[],"source":["test(model, test_loader, batch_size=batch_size)"]},{"cell_type":"code","execution_count":null,"id":"b8099b3d-66ab-44cf-ae20-9357dbcd9bce","metadata":{"id":"b8099b3d-66ab-44cf-ae20-9357dbcd9bce"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"85f2a1d6-d4ea-49da-9c71-dc95412bd6d1","metadata":{"id":"85f2a1d6-d4ea-49da-9c71-dc95412bd6d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"e722f5da-44a4-4da5-9dcc-f7fe4667250b","metadata":{"id":"e722f5da-44a4-4da5-9dcc-f7fe4667250b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"15548126-a383-40a7-b21f-2fb070b1a3ff","metadata":{"id":"15548126-a383-40a7-b21f-2fb070b1a3ff"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"ced1915b-2d74-4f71-a85b-598c09c9ffa0","metadata":{"id":"ced1915b-2d74-4f71-a85b-598c09c9ffa0"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"3bf4800c-a9f9-4615-ab5b-dfdbb9e9d4d1","metadata":{"id":"3bf4800c-a9f9-4615-ab5b-dfdbb9e9d4d1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"0e847b73-0262-4f44-a083-5291bd21dd9a","metadata":{"id":"0e847b73-0262-4f44-a083-5291bd21dd9a"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"cd794263-bf65-42b9-be1b-f7539478fd6f","metadata":{"id":"cd794263-bf65-42b9-be1b-f7539478fd6f"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"cf966bb1-ea15-458d-b2c8-03df4a54c2c1","metadata":{"id":"cf966bb1-ea15-458d-b2c8-03df4a54c2c1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"dd6840a9-ec02-42a4-bcc9-c9e50e069079","metadata":{"id":"dd6840a9-ec02-42a4-bcc9-c9e50e069079"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"fded8e03-6f8e-428f-b5c4-5c57d400a856","metadata":{"id":"fded8e03-6f8e-428f-b5c4-5c57d400a856"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b89544f9-d1a1-45fb-bcc9-89385f39190b","metadata":{"id":"b89544f9-d1a1-45fb-bcc9-89385f39190b"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"932fe51b-03ef-47c2-89d0-857db68b2326","metadata":{"id":"932fe51b-03ef-47c2-89d0-857db68b2326"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"080afcc6-f3b1-4421-961b-bf8ee4dc28f4","metadata":{"id":"080afcc6-f3b1-4421-961b-bf8ee4dc28f4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"9445384f-fd70-468c-85be-51b3d7a97ab4","metadata":{"id":"9445384f-fd70-468c-85be-51b3d7a97ab4"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"d25a0bb8-d2aa-4746-a1a9-080ec7d1d4f1","metadata":{"id":"d25a0bb8-d2aa-4746-a1a9-080ec7d1d4f1"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"id":"b9b931f6-0891-4879-9790-ba03def15c77","metadata":{"id":"b9b931f6-0891-4879-9790-ba03def15c77"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.18"}},"nbformat":4,"nbformat_minor":5}
